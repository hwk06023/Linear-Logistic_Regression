# Linear-Logistic_Regression

수치 예측을 위한 선형 회귀 / 이진 분류를 위한 로지스틱 회귀를 배워봅니다.

## Linear Regression

먼저 선형 회귀는 우리가 배운 1차 함수와 비교하여 간단하게 파악할 수 있습니다. <br/>

<pre><code>y = ax + b</code></pre>

위의 식은 우리가 흔히 볼 수 있는 1차 함수의 식입니다. <br/>
저희는 위 식에서 'a'를 기울기 'b'를 절편이라고 학교에서 배웠었죠. <br/>

그리고 기울기(a)와 절편(b)이 각각 주어진 상태에서 저희는 'y'값에 따른 'x'값을 구했었습니다. <br/>

<pre><code>y = 7x + 4</code></pre>

예를 들면, 위의 식은 기울기가 7, 절편이 4인 1차 함수입니다. <br/>
위 식에서 y = 25라면? x = 3임을 구하는 것이 저희가 배웠었던 1차 함수의 문제 해결이죠. <br/>

그렇다면 이제 본격적으로 선형 회귀의 문제 해결에 대해 알아봅시다. <br/>

<pre><code>y = ax + b</code></pre>

다시 한 번 위에서의 1차 함수 식으로 돌아가봅시다. <br/>
이번에는 반대로 'y'값에 따른 'x'값들이 주어진 상태에서, 기울기(a)와 절편(b)를 구해볼겁니다. <br/>





  - - -

## Logistic Regression

